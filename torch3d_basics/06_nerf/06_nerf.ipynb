{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import open3d\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from pytorch3d.structures import Pointclouds, Volumes\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    BlendParams,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    FoVPerspectiveCameras,\n",
    "    ImplicitRenderer,\n",
    "    look_at_view_transform,\n",
    "    MeshRasterizer,\n",
    "    MeshRenderer,\n",
    "    MonteCarloRaysampler,\n",
    "    NDCGridRaysampler,\n",
    "    NDCMultinomialRaysampler,\n",
    "    PointLights,\n",
    "    RasterizationSettings,\n",
    "    ray_bundle_to_ray_points,\n",
    "    RayBundle,\n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    VolumeRenderer,\n",
    "    VolumeSampler,\n",
    ")\n",
    "\n",
    "\n",
    "def delete_from_disk(path: pathlib.Path):\n",
    "    if path.is_file() or path.is_symlink():\n",
    "        path.unlink()\n",
    "        return\n",
    "    for p in path.iterdir():\n",
    "        delete_from_disk(p)\n",
    "    path.rmdir()\n",
    "\n",
    "\n",
    "out_path = os.path.abspath(\"./out\")\n",
    "if os.path.isdir(out_path):\n",
    "    delete_from_disk(pathlib.Path(out_path))\n",
    "os.mkdir(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_point_cloud(point_cloud):\n",
    "    pcd = open3d.io.read_point_cloud(point_cloud)\n",
    "    open3d.visualization.draw_geometries([pcd], mesh_show_wireframe=True)\n",
    "\n",
    "\n",
    "def visualize_point_cloud2(verts, device, renderer):\n",
    "    d_verts = (verts).to(device)\n",
    "    rgb = torch.ones_like(d_verts).to(device)\n",
    "    point_cloud = Pointclouds(points=[verts], features=[rgb])\n",
    "    images = renderer(point_cloud)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def visualize_point_cloud2_save(verts, device, renderer, index):\n",
    "    d_verts = (verts).to(device)\n",
    "    rgb = torch.ones_like(d_verts).to(device)\n",
    "    point_cloud = Pointclouds(points=[verts], features=[rgb])\n",
    "    images = renderer(point_cloud)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"./out/image{index}.png\")\n",
    "\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "    print(f\"Visualizing the mesh {mesh} using open3D\")\n",
    "    mesh = open3d.io.read_triangle_mesh(mesh, enable_post_processing=True)\n",
    "    open3d.visualization.draw_geometries([mesh], mesh_show_wireframe=True, mesh_show_back_face=True)\n",
    "\n",
    "\n",
    "def image_grid(images, rows=None, cols=None, fill: bool = True, show_axes: bool = False, rgb: bool = True):\n",
    "    \"\"\"\n",
    "    A util function for plotting a grid of images.\n",
    "\n",
    "    Args:\n",
    "        images: (N, H, W, 4) array of RGBA images\n",
    "        rows: number of rows in the grid\n",
    "        cols: number of columns in the grid\n",
    "        fill: boolean indicating if the space between images should be filled\n",
    "        show_axes: boolean indicating if the axes of the plots should be visible\n",
    "        rgb: boolean, If True, only RGB channels are plotted.\n",
    "            If False, only the alpha channel is plotted.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if (rows is None) != (cols is None):\n",
    "        raise ValueError(\"Specify either both rows and cols or neither.\")\n",
    "\n",
    "    if rows is None:\n",
    "        rows = len(images)\n",
    "        cols = 1\n",
    "\n",
    "    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n",
    "    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n",
    "    bleed = 0\n",
    "    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n",
    "\n",
    "    for ax, im in zip(axarr.ravel(), images):\n",
    "        if rgb:\n",
    "            # only render RGB channels\n",
    "            ax.imshow(im[..., :3])\n",
    "        else:\n",
    "            # only render Alpha channel\n",
    "            ax.imshow(im[..., 3])\n",
    "        if not show_axes:\n",
    "            ax.set_axis_off()\n",
    "\n",
    "\n",
    "def generate_cow_renders(num_views: int = 40, azimuth_range: float = 180):\n",
    "    \"\"\"\n",
    "    This function generates `num_views` renders of a cow mesh.\n",
    "    The renders are generated from viewpoints sampled at uniformly distributed\n",
    "    azimuth intervals. The elevation is kept constant so that the camera's\n",
    "    vertical position coincides with the equator.\n",
    "\n",
    "    For a more detailed explanation of this code, please refer to the\n",
    "    docs/tutorials/fit_textured_mesh.ipynb notebook.\n",
    "\n",
    "    Args:\n",
    "        num_views: The number of generated renders.\n",
    "        data_dir: The folder that contains the cow mesh files. If the cow mesh\n",
    "            files do not exist in the folder, this function will automatically\n",
    "            download them.\n",
    "\n",
    "    Returns:\n",
    "        cameras: A batch of `num_views` `FoVPerspectiveCameras` from which the\n",
    "            images are rendered.\n",
    "        images: A tensor of shape `(num_views, height, width, 3)` containing\n",
    "            the rendered images.\n",
    "        silhouettes: A tensor of shape `(num_views, height, width)` containing\n",
    "            the rendered silhouettes.\n",
    "    \"\"\"\n",
    "\n",
    "    # set the paths\n",
    "    spot_dir = os.path.abspath(\"./spot\")\n",
    "\n",
    "    # Setup\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Load obj file\n",
    "    obj_filename = os.path.join(spot_dir, \"spot.obj\")\n",
    "    mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "    # We scale normalize and center the target mesh to fit in a sphere of radius 1\n",
    "    # centered at (0,0,0). (scale, center) will be used to bring the predicted mesh\n",
    "    # to its original center and scale.  Note that normalizing the target mesh,\n",
    "    # speeds up the optimization but is not necessary!\n",
    "    verts = mesh.verts_packed()\n",
    "    N = verts.shape[0]\n",
    "    center = verts.mean(0)\n",
    "    scale = max((verts - center).abs().max(0)[0])\n",
    "    mesh.offset_verts_(-(center.expand(N, 3)))\n",
    "    mesh.scale_verts_((1.0 / float(scale)))\n",
    "\n",
    "    # Get a batch of viewing angles.\n",
    "    elev = torch.linspace(0, 0, num_views)  # keep constant\n",
    "    azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0\n",
    "\n",
    "    # Place a point light in front of the object. As mentioned above, the front of\n",
    "    # the cow is facing the -z direction.\n",
    "    lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "    # Initialize an OpenGL perspective camera that represents a batch of different\n",
    "    # viewing angles. All the cameras helper methods support mixed type inputs and\n",
    "    # broadcasting. So we can view the camera from the a distance of dist=2.7, and\n",
    "    # then specify elevation and azimuth angles for each viewpoint as tensors.\n",
    "    R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "    # Define the settings for rasterization and shading. Here we set the output\n",
    "    # image to be of size 128X128. As we are rendering images for visualization\n",
    "    # purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to\n",
    "    # rasterize_meshes.py for explanations of these parameters.  We also leave\n",
    "    # bin_size and max_faces_per_bin to their default values of None, which sets\n",
    "    # their values using heuristics and ensures that the faster coarse-to-fine\n",
    "    # rasterization method is used.  Refer to docs/notes/renderer.md for an\n",
    "    # explanation of the difference between naive and coarse-to-fine rasterization.\n",
    "    raster_settings = RasterizationSettings(image_size=128, blur_radius=0.0, faces_per_pixel=1)\n",
    "\n",
    "    # Create a Phong renderer by composing a rasterizer and a shader. The textured\n",
    "    # Phong shader will interpolate the texture uv coordinates for each vertex,\n",
    "    # sample from a texture image and apply the Phong lighting model\n",
    "    blend_params = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(0.0, 0.0, 0.0))\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
    "        shader=SoftPhongShader(device=device, cameras=cameras, lights=lights, blend_params=blend_params),\n",
    "    )\n",
    "\n",
    "    # Create a batch of meshes by repeating the cow mesh and associated textures.\n",
    "    # Meshes has a useful `extend` method which allows us do this very easily.\n",
    "    # This also extends the textures.\n",
    "    meshes = mesh.extend(num_views)\n",
    "\n",
    "    # Render the cow mesh from each viewing angle\n",
    "    target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "    # Rasterization settings for silhouette rendering\n",
    "    sigma = 1e-4\n",
    "    raster_settings_silhouette = RasterizationSettings(image_size=128, blur_radius=numpy.log(1.0 / 1e-4 - 1.0) * sigma, faces_per_pixel=50)\n",
    "\n",
    "    # Silhouette renderer\n",
    "    renderer_silhouette = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings_silhouette),\n",
    "        shader=SoftSilhouetteShader(),\n",
    "    )\n",
    "\n",
    "    # Render silhouette images.  The 3rd channel of the rendering output is\n",
    "    # the alpha/silhouette channel\n",
    "    silhouette_images = renderer_silhouette(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "    # binary silhouettes\n",
    "    silhouette_binary = (silhouette_images[..., 3] > 1e-4).float()\n",
    "\n",
    "    return cameras, target_images[..., :3], silhouette_binary, silhouette_images\n",
    "\n",
    "\n",
    "def huber(x, y, scaling=0.1):\n",
    "    \"\"\"\n",
    "    A helper function for evaluating the smooth L1 (huber) loss\n",
    "    between the rendered silhouettes and colors.\n",
    "    \"\"\"\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    \"\"\"\n",
    "    Given a set of Monte Carlo pixel locations `sampled_rays_xy`,\n",
    "    this method samples the tensor `target_images` at the\n",
    "    respective 2D locations.\n",
    "\n",
    "    This function is used in order to extract the colors from\n",
    "    ground truth images that correspond to the colors\n",
    "    rendered using `MonteCarloRaysampler`.\n",
    "    \"\"\"\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    # In order to sample target_images, we utilize\n",
    "    # the grid_sample function which implements a\n",
    "    # bilinear image sampler.\n",
    "    # Note that we have to invert the sign of the\n",
    "    # sampled ray positions to convert the NDC xy locations\n",
    "    # of the MonteCarloRaysampler to the coordinate\n",
    "    # convention of grid_sample.\n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2), -sampled_rays_xy.view(ba, -1, 1, 2), align_corners=True  # note the sign inversion\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(ba, *spatial_size, dim)\n",
    "\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field,\n",
    "    camera,\n",
    "    target_image,\n",
    "    target_silhouette,\n",
    "    renderer_grid,\n",
    "    loss_history_color,\n",
    "    loss_history_sil,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is a helper function for visualizing the\n",
    "    intermediate results of the learning.\n",
    "\n",
    "    Since the `NeuralRadianceField` suffers from\n",
    "    a large memory footprint, which does not let us\n",
    "    render the full image grid in a single forward pass,\n",
    "    we utilize the `NeuralRadianceField.batched_forward`\n",
    "    function in combination with disabling the gradient caching.\n",
    "    This chunks the set of emitted rays to batches and\n",
    "    evaluates the implicit function on one batch at a time\n",
    "    to prevent GPU memory overflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prevent gradient caching.\n",
    "    with torch.no_grad():\n",
    "        # Render using the grid renderer and the\n",
    "        # batched_forward function of neural_radiance_field.\n",
    "        rendered_image_silhouette, _ = renderer_grid(cameras=camera, volumetric_function=neural_radiance_field.batched_forward)\n",
    "        # Split the rendering result to a silhouette render\n",
    "        # and the image render.\n",
    "        rendered_image, rendered_silhouette = rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "\n",
    "    # Generate plots.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\",\n",
    "            \"rendered image\",\n",
    "            \"rendered silhouette\",\n",
    "            \"loss silhouette\",\n",
    "            \"target image\",\n",
    "            \"target silhouette\",\n",
    "        ),\n",
    "    ):\n",
    "        if not title_.startswith(\"loss\"):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def generate_rotating_nerf(neural_radiance_field, target_cameras, renderer_grid, n_frames=50, device=torch.device(\"cpu\")):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print(\"Rendering rotating NeRF ...\")\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None],\n",
    "            T=T[None],\n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Note that we again render with `NDCMultinomialRaysampler`\n",
    "        # and the batched_forward function of neural_radiance_field.\n",
    "        frames.append(\n",
    "            renderer_grid(\n",
    "                cameras=camera,\n",
    "                volumetric_function=neural_radiance_field.batched_forward,\n",
    "            )[\n",
    "                0\n",
    "            ][..., :3]\n",
    "        )\n",
    "    return torch.cat(frames)\n",
    "\n",
    "\n",
    "def huber(x, y, scaling=0.1):\n",
    "    \"\"\"\n",
    "    A helper function for evaluating the smooth L1 (huber) loss\n",
    "    between the rendered silhouettes and colors.\n",
    "    \"\"\"\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    \"\"\"\n",
    "    Given a set of Monte Carlo pixel locations `sampled_rays_xy`,\n",
    "    this method samples the tensor `target_images` at the\n",
    "    respective 2D locations.\n",
    "\n",
    "    This function is used in order to extract the colors from\n",
    "    ground truth images that correspond to the colors\n",
    "    rendered using `MonteCarloRaysampler`.\n",
    "    \"\"\"\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    # In order to sample target_images, we utilize\n",
    "    # the grid_sample function which implements a\n",
    "    # bilinear image sampler.\n",
    "    # Note that we have to invert the sign of the\n",
    "    # sampled ray positions to convert the NDC xy locations\n",
    "    # of the MonteCarloRaysampler to the coordinate\n",
    "    # convention of grid_sample.\n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2), -sampled_rays_xy.view(ba, -1, 1, 2), align_corners=True  # note the sign inversion\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(ba, *spatial_size, dim)\n",
    "\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field,\n",
    "    camera,\n",
    "    target_image,\n",
    "    target_silhouette,\n",
    "    renderer_grid,\n",
    "    loss_history_color,\n",
    "    loss_history_sil,\n",
    "):\n",
    "    \"\"\"\n",
    "    This is a helper function for visualizing the\n",
    "    intermediate results of the learning.\n",
    "\n",
    "    Since the `NeuralRadianceField` suffers from\n",
    "    a large memory footprint, which does not let us\n",
    "    render the full image grid in a single forward pass,\n",
    "    we utilize the `NeuralRadianceField.batched_forward`\n",
    "    function in combination with disabling the gradient caching.\n",
    "    This chunks the set of emitted rays to batches and\n",
    "    evaluates the implicit function on one batch at a time\n",
    "    to prevent GPU memory overflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prevent gradient caching.\n",
    "    with torch.no_grad():\n",
    "        # Render using the grid renderer and the\n",
    "        # batched_forward function of neural_radiance_field.\n",
    "        rendered_image_silhouette, _ = renderer_grid(cameras=camera, volumetric_function=neural_radiance_field.batched_forward)\n",
    "        # Split the rendering result to a silhouette render\n",
    "        # and the image render.\n",
    "        rendered_image, rendered_silhouette = rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "\n",
    "    # Generate plots.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\",\n",
    "            \"rendered image\",\n",
    "            \"rendered silhouette\",\n",
    "            \"loss silhouette\",\n",
    "            \"target image\",\n",
    "            \"target silhouette\",\n",
    "        ),\n",
    "    ):\n",
    "        if not title_.startswith(\"loss\"):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def generate_rotating_nerf(neural_radiance_field, target_cameras, renderer_grid, n_frames=50, device=torch.device(\"cpu\")):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print(\"Rendering rotating NeRF ...\")\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None],\n",
    "            T=T[None],\n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Note that we again render with `NDCMultinomialRaysampler`\n",
    "        # and the batched_forward function of neural_radiance_field.\n",
    "        frames.append(\n",
    "            renderer_grid(\n",
    "                cameras=camera,\n",
    "                volumetric_function=neural_radiance_field.batched_forward,\n",
    "            )[\n",
    "                0\n",
    "            ][..., :3]\n",
    "        )\n",
    "    return torch.cat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        \"\"\"\n",
    "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
    "        the harmonic embedding layer converts each feature\n",
    "        in `x` into a series of harmonic features `embedding`\n",
    "        as follows:\n",
    "            embedding[..., i*dim:(i+1)*dim] = [\n",
    "                sin(x[..., i]),\n",
    "                sin(2*x[..., i]),\n",
    "                sin(4*x[..., i]),\n",
    "                ...\n",
    "                sin(2**(self.n_harmonic_functions-1) * x[..., i]),\n",
    "                cos(x[..., i]),\n",
    "                cos(2*x[..., i]),\n",
    "                cos(4*x[..., i]),\n",
    "                ...\n",
    "                cos(2**(self.n_harmonic_functions-1) * x[..., i])\n",
    "            ]\n",
    "\n",
    "        Note that `x` is also premultiplied by `omega0` before\n",
    "        evaluating the harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            \"frequencies\",\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "        Returns:\n",
    "            embedding: a harmonic embedding of `x`\n",
    "                of shape [..., n_harmonic_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
    "\n",
    "\n",
    "class NeuralRadianceField(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_harmonic_functions: The number of harmonic functions\n",
    "                used to form the harmonic embedding of each point.\n",
    "            n_hidden_neurons: The number of hidden units in the\n",
    "                fully connected layers of the MLPs of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # The harmonic embedding layer converts input 3D coordinates\n",
    "        # to a representation that is more suitable for\n",
    "        # processing with a deep neural network.\n",
    "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
    "\n",
    "        # The dimension of the harmonic embedding.\n",
    "        embedding_dim = n_harmonic_functions * 2 * 3\n",
    "\n",
    "        # self.mlp is a simple 2-layer multi-layer perceptron\n",
    "        # which converts the input per-point harmonic embeddings\n",
    "        # to a latent representation.\n",
    "        # Not that we use Softplus activations instead of ReLU.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "        )\n",
    "\n",
    "        # Given features predicted by self.mlp, self.color_layer\n",
    "        # is responsible for predicting a 3-D per-point vector\n",
    "        # that represents the RGB color of the point.\n",
    "        self.color_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, 3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # To ensure that the colors correctly range between [0-1],\n",
    "            # the layer is terminated with a sigmoid layer.\n",
    "        )\n",
    "\n",
    "        # The density layer converts the features of self.mlp\n",
    "        # to a 1D density value representing the raw opacity\n",
    "        # of each point.\n",
    "        self.density_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons, 1),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            # Sofplus activation ensures that the raw opacity\n",
    "            # is a non-negative number.\n",
    "        )\n",
    "\n",
    "        # We set the bias of the density layer to -1.5\n",
    "        # in order to initialize the opacities of the\n",
    "        # ray points to values close to 0.\n",
    "        # This is a crucial detail for ensuring convergence\n",
    "        # of the model.\n",
    "        self.density_layer[0].bias.data[0] = -1.5\n",
    "\n",
    "    def _get_densities(self, features):\n",
    "        \"\"\"\n",
    "        This function takes `features` predicted by `self.mlp`\n",
    "        and converts them to `raw_densities` with `self.density_layer`.\n",
    "        `raw_densities` are later mapped to [0-1] range with\n",
    "        1 - inverse exponential of `raw_densities`.\n",
    "        \"\"\"\n",
    "        raw_densities = self.density_layer(features)\n",
    "        return 1 - (-raw_densities).exp()\n",
    "\n",
    "    def _get_colors(self, features, rays_directions):\n",
    "        \"\"\"\n",
    "        This function takes per-point `features` predicted by `self.mlp`\n",
    "        and evaluates the color model in order to attach to each\n",
    "        point a 3D vector of its RGB color.\n",
    "\n",
    "        In order to represent viewpoint dependent effects,\n",
    "        before evaluating `self.color_layer`, `NeuralRadianceField`\n",
    "        concatenates to the `features` a harmonic embedding\n",
    "        of `ray_directions`, which are per-point directions\n",
    "        of point rays expressed as 3D l2-normalized vectors\n",
    "        in world coordinates.\n",
    "        \"\"\"\n",
    "        spatial_size = features.shape[:-1]\n",
    "\n",
    "        # Normalize the ray_directions to unit l2 norm.\n",
    "        rays_directions_normed = torch.nn.functional.normalize(rays_directions, dim=-1)\n",
    "\n",
    "        # Obtain the harmonic embedding of the normalized ray directions.\n",
    "        rays_embedding = self.harmonic_embedding(rays_directions_normed)\n",
    "\n",
    "        # Expand the ray directions tensor so that its spatial size\n",
    "        # is equal to the size of features.\n",
    "        rays_embedding_expand = rays_embedding[..., None, :].expand(*spatial_size, rays_embedding.shape[-1])\n",
    "\n",
    "        # Concatenate ray direction embeddings with\n",
    "        # features and evaluate the color model.\n",
    "        color_layer_input = torch.cat((features, rays_embedding_expand), dim=-1)\n",
    "        return self.color_layer(color_layer_input)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ray_bundle: RayBundle,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The forward function accepts the parametrizations of\n",
    "        3D points sampled along projection rays. The forward\n",
    "        pass is responsible for attaching a 3D vector\n",
    "        and a 1D scalar representing the point's\n",
    "        RGB color and opacity respectively.\n",
    "\n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "        \"\"\"\n",
    "        # We first convert the ray parametrizations to world\n",
    "        # coordinates with `ray_bundle_to_ray_points`.\n",
    "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
    "        # rays_points_world.shape = [minibatch x ... x 3]\n",
    "\n",
    "        # For each 3D world coordinate, we obtain its harmonic embedding.\n",
    "        embeds = self.harmonic_embedding(rays_points_world)\n",
    "        # embeds.shape = [minibatch x ... x self.n_harmonic_functions*6]\n",
    "\n",
    "        # self.mlp maps each harmonic embedding to a latent feature space.\n",
    "        features = self.mlp(embeds)\n",
    "        # features.shape = [minibatch x ... x n_hidden_neurons]\n",
    "\n",
    "        # Finally, given the per-point features,\n",
    "        # execute the density and color branches.\n",
    "\n",
    "        rays_densities = self._get_densities(features)\n",
    "        # rays_densities.shape = [minibatch x ... x 1]\n",
    "\n",
    "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
    "        # rays_colors.shape = [minibatch x ... x 3]\n",
    "\n",
    "        return rays_densities, rays_colors\n",
    "\n",
    "    def batched_forward(\n",
    "        self,\n",
    "        ray_bundle: RayBundle,\n",
    "        n_batches: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function is used to allow for memory efficient processing\n",
    "        of input rays. The input rays are first split to `n_batches`\n",
    "        chunks and passed through the `self.forward` function one at a time\n",
    "        in a for loop. Combined with disabling PyTorch gradient caching\n",
    "        (`torch.no_grad()`), this allows for rendering large batches\n",
    "        of rays that do not all fit into GPU memory in a single forward pass.\n",
    "        In our case, batched_forward is used to export a fully-sized render\n",
    "        of the radiance field for visualization purposes.\n",
    "\n",
    "        Args:\n",
    "            ray_bundle: A RayBundle object containing the following variables:\n",
    "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
    "                    origins of the sampling rays in world coords.\n",
    "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
    "                    containing the direction vectors of sampling rays in world coords.\n",
    "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
    "                    containing the lengths at which the rays are sampled.\n",
    "            n_batches: Specifies the number of batches the input rays are split into.\n",
    "                The larger the number of batches, the smaller the memory footprint\n",
    "                and the lower the processing speed.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Parse out shapes needed for tensor reshaping in this function.\n",
    "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
    "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
    "\n",
    "        # Split the rays to `n_batches` batches.\n",
    "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
    "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
    "\n",
    "        # For each batch, execute the standard forward pass.\n",
    "        batch_outputs = [\n",
    "            self.forward(\n",
    "                RayBundle(\n",
    "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
    "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
    "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
    "                    xys=None,\n",
    "                )\n",
    "            )\n",
    "            for batch_idx in batches\n",
    "        ]\n",
    "\n",
    "        # Concatenate the per-batch rays_densities and rays_colors\n",
    "        # and reshape according to the sizes of the inputs.\n",
    "        rays_densities, rays_colors = [\n",
    "            torch.cat([batch_output[output_i] for batch_output in batch_outputs], dim=0).view(*spatial_size, -1) for output_i in (0, 1)\n",
    "        ]\n",
    "        return rays_densities, rays_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 images/silhouettes/cameras.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "target_cameras, target_images, target_silhouettes, silhouette_images = generate_cow_renders(num_views=40, azimuth_range=180)\n",
    "\n",
    "for i, t_image in enumerate(target_images):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(t_image.cpu().detach().numpy().squeeze())\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(out_path, f\"starting_rgb_{i}.png\"))\n",
    "    plt.axis(\"off\")\n",
    "    plt.close()\n",
    "\n",
    "for i, s_image in enumerate(silhouette_images):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(s_image.cpu().detach().numpy().squeeze()[..., 3])\n",
    "    plt.grid(False)\n",
    "    plt.savefig(os.path.join(out_path, f\"starting_silhouette_{i}.png\"))\n",
    "    plt.axis(\"off\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Generated {len(target_images)} images/silhouettes/cameras.\")\n",
    "\n",
    "render_size = target_images.shape[1] * 2\n",
    "\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "raysampler_mc = MonteCarloRaysampler(\n",
    "    min_x=-1.0,\n",
    "    max_x=1.0,\n",
    "    min_y=-1.0,\n",
    "    max_y=1.0,\n",
    "    n_rays_per_image=750,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "renderer_mc = ImplicitRenderer(raysampler=raysampler_mc, raymarcher=raymarcher)\n",
    "\n",
    "render_size = target_images.shape[1] * 2\n",
    "volume_extent_world = 3.0\n",
    "raysampler_grid = NDCMultinomialRaysampler(\n",
    "    image_height=render_size, image_width=render_size, n_pts_per_ray=128, min_depth=0.1, max_depth=volume_extent_world\n",
    ")\n",
    "\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "renderer_grid = ImplicitRenderer(raysampler=raysampler_grid, raymarcher=raymarcher)\n",
    "\n",
    "neural_radiance_field = NeuralRadianceField()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "renderer_grid = renderer_grid.to(device)\n",
    "renderer_mc = renderer_mc.to(device)\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "neural_radiance_field = neural_radiance_field.to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
    "batch_size = 6\n",
    "n_iter = 3000\n",
    "\n",
    "loss_history_color, loss_history_sil = [], []\n",
    "for iteration in range(n_iter):\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print(\"Decreasing LR 10-fold ...\")\n",
    "        optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr * 0.1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R=target_cameras.R[batch_idx],\n",
    "        T=target_cameras.T[batch_idx],\n",
    "        znear=target_cameras.znear[batch_idx],\n",
    "        zfar=target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
    "        fov=target_cameras.fov[batch_idx],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    rendered_images_silhouettes, sampled_rays = renderer_mc(cameras=batch_cameras, volumetric_function=neural_radiance_field)\n",
    "\n",
    "    rendered_images, rendered_silhouettes = rendered_images_silhouettes.split([3, 1], dim=-1)\n",
    "\n",
    "    silhouettes_at_rays = sample_images_at_mc_locs(target_silhouettes[batch_idx, ..., None], sampled_rays.xys)\n",
    "\n",
    "    sil_err = (\n",
    "        huber(\n",
    "            rendered_silhouettes,\n",
    "            silhouettes_at_rays,\n",
    "        )\n",
    "        .abs()\n",
    "        .mean()\n",
    "    )\n",
    "    colors_at_rays = sample_images_at_mc_locs(target_images[batch_idx], sampled_rays.xys)\n",
    "\n",
    "    color_err = (\n",
    "        huber(\n",
    "            rendered_images,\n",
    "            colors_at_rays,\n",
    "        )\n",
    "        .abs()\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    loss = color_err + sil_err\n",
    "    loss_history_color.append(float(color_err))\n",
    "    loss_history_sil.append(float(sil_err))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Visualize the full renders every 100 iterations.\n",
    "    if iteration % 100 == 0:\n",
    "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
    "        fig = show_full_render(\n",
    "            neural_radiance_field,\n",
    "            FoVPerspectiveCameras(\n",
    "                R=target_cameras.R[show_idx],\n",
    "                T=target_cameras.T[show_idx],\n",
    "                znear=target_cameras.znear[show_idx],\n",
    "                zfar=target_cameras.zfar[show_idx],\n",
    "                aspect_ratio=target_cameras.aspect_ratio[show_idx],\n",
    "                fov=target_cameras.fov[show_idx],\n",
    "                device=device,\n",
    "            ),\n",
    "            target_images[show_idx][0],\n",
    "            target_silhouettes[show_idx][0],\n",
    "            renderer_grid,\n",
    "            loss_history_color,\n",
    "            loss_history_sil,\n",
    "        )\n",
    "        fig.savefig(f\"{out_path}/intermediate_{iteration}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, target_cameras, renderer_grid, n_frames=3 * 5 * 20, device=device)\n",
    "\n",
    "# image_grid(rotating_nerf_frames.clamp(0.0, 1.0).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True)\n",
    "# plt.show()\n",
    "\n",
    "print(\"Saving animation\")\n",
    "frames = []\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for frame in rotating_nerf_frames:\n",
    "    frames.append([plt.imshow(frame.clamp(0.0, 1.0).cpu().numpy(), animated=True)])\n",
    "\n",
    "anim = animation.ArtistAnimation(fig, frames, interval=50, blit=True, repeat_delay=1000)\n",
    "anim.save(os.path.join(out_path, \"rotating_volumet_animation.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
